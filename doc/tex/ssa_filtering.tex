\subsection{Background}
	Singular Spectrum Analysis (SSA) \cite{Ghil02} was originally developed as a model-free technique to decompose a time series (including a non-stationary time series) into the sum of interpretable componets such as trend, periodic components, and noise, with no a-priori assumptions about the form of those components. For a full derivation of the 2D treatment, see \cite{Golyandina13} however the 1D form is fairly straight forward and has the same basic steps.

\subsection{Why go to the trouble?}
The problem I was trying to solve when I started using this technique was this: How can I reduce/flag instrumental artifacts that vary in position and intesity between wavelength channels without laboriously going through each image? I tried a few different techniques.

Low pass filtering had quite a few problems. It did reduce smaller artifacts, but did not affect larger ones that are extended in one dimension but not another. Furthermore, it mostly just blurred the observational data which is the exact opposite of what we are trying to achive with the whole process, namely removing blur to get good estimates of limb-darkening parameters.

Edge detection via different methods (e.g. Sobel filtering) was moderately successful, however all the techniques I found preferentially find edges along certain directions. That wasn't too much of a problem, as the routines are cheap (computationally) and can be run multiple times, but they also missed obvious artifacts and flagged non-artifact. I.e., their false positive and false negative rate are quite high.

I had a look into computer vision libraries to see if I could use motion estimation techniques like Optical Flow or Block Matching, as many artifacts move across the imaged field w.r.t.~wavelength. However, this had a couple of problems. One, not all of the artifacts move and I would still have to find some way to deal with them. Two, I was starting to veer into having to learn a lot about computer vision which was probably just as much work as going through the images manually (at least for one observation).

SSA was suggested to me by a friend, and while it is certainly not perfect, it has a good balance of flagging obvious artifacts, not flagging actual data, and being fairly computationally efficient. In the implementation I'm using it's not using any wavelength-varying information, so there's probably a way to improve it (3D SSA?) or use a secondary technique to identify artifacts that move with wavelength.

\subsubsection{Singular Value Decomposition}
SSA uses Singular Value Decomposition (SVD) to split up a series into components, here's a quick overview of it. SVD factorises an $m \times n$ matrix into an orthogonal eigenbasis.

\[
	\mat{M} = \mat{U}\mat{\Sigma}\mat{V}^\star
\]
\begin{center}
\begin{tabular}{l|l}
 Matrix 			& Shape  			\\
 \hline	
 $\mat{M}$ 		& $(m \times n)$ 	\\
 $\mat{U}$ 		& $(m \times m)$ 	\\
 $\mat{\Sigma}$ 	& $(m \times n)$   	\\
 $\mat{V}$ 		& $(n \times n)$ 
\end{tabular}
\end{center}

$\mat{M}$ is the matrix to factorise, $\mat{U}$ is the left singular matrix, $\mat{\Sigma}$ holds the singular values along its diagonal, and $\mat{V}$ is the right singular matrix. $\mat{U}$ and $\mat{V}$ are \emph{unitary}, their conjugate transpose is their inverse, and $\mat{\Sigma}$ is a diagonal matrix with non-negative real numbers on its diagonal.

SVD gives a non-unique solution, however we can always choose $\Sigma_{ii}$'s to be in decending order. In which case $\mat{\Sigma}$ is unique, but $\mat{U}$ and $\mat{V}$ are not.

Sometimes the compact form of SVD is used, which only includes the non-zero terms in $\mat{\Sigma}$, in this case the shapes of the matrices are changes such that $\mat{U}$ is shape $(m \times r)$, $\mat{\Sigma}$ is shape $(r \times r)$, and $\mat{V}$ is shape $(r \times n)$. In this case $\mat{U}$ and $\mat{V}$ are \emph{semi unitary}, and $r$ is the number of non-zero elements in the original $\mat{\sigma}$.


\subsubsection{Method}
Let $\mat{\mathbb{X}}$ be a real-values time series, $\mat{\mathbb{X}} = (x_1, x_2, ..., x_n)$ of length $N$, let $L$ (with $1<L<N$) be an integer called the \emph{window length}, and $K = N - L + 1$.

First form the \emph{trajectory matrix} $\mat{X}$, the $L \times K$ matrix that holds each moving window of length L in its columns.
\[
	\mat{X} = \begin{pmatrix}
	x_1 & x_2 & ... & x_K \\
	x_2 & x_3 & ... & x_{K+1} \\
	\vdots & \vdots & \ddots & \vdots \\
	x_L & x_{L+1} & ... & x_{N} 
	\end{pmatrix}
\]
$\mat{X}$ is a \emph{Hankel} matrix, i.e., all its anti-diagonals have constant elements.

Secondly, perform SVD on $\mat{X}$. A slightly faster way than the direct version is the following. Compute
\begin{align*}
	\mat{S} &= \mat{X}\mat{X}^\textrm{T},
\end{align*}
S is of shape $L \times L$, and let $\lambda_1, \lambda_2,...,\lambda_L$ be the \emph{eigenvalues} of $\mat{S}$. Therefore,
\begin{align*}
	\mat{S} &= (\mat{U}\mat{\Sigma}\mat{V}^\star)(\mat{U}\mat{\Sigma}\mat{V}^\star)^\star \\
		&= \mat{U}\mat{\Sigma}\mat{V}^\star\mat{V}\mat{\Sigma}^\star\mat{U}^\star \\
		&= \mat{U}\mat{\Sigma}^2\mat{U}
\end{align*}
the eigenvalues of $\mat{S}$ are the squares of the singular values of $\mat{X}$, and the eigenvectors of $\mat{S}$ are the columns of the left singular matrix $\mat{U}$ of $\mat{X}$. Also,
\begin{align*}
	\mat{X}^\star &= \mat{V} \mat{\Sigma}^\star \mat{U}^\star \\
	\rightarrow \quad \mat{X}^\star \mat{U} \mat{\Sigma}^{-1} &= \mat{V} 
\end{align*}
means that the right singular matrices $\mat{V}$ of $\mat{X}$ can be found via
\begin{align*}
	V_i = \frac{\mat{X}^\star \mat{U}}{\sqrt{\lambda_i}}.
\end{align*}

So, the SVD can be decomposed into a sum via
\begin{align*}
	\mat{U} &= (U_1, U_2, ..., U_L) \\
	\mat{V} &= (V_1, V_2, ..., V_K) \\
	\textrm{let} \quad d &= \textrm{min}(L,K) \\
	\mat{X} &= (U_1, U_2, ..., U_L) \mat{\Sigma} (V_1, V_2, ..., V_K)^\star \\
			&= (U_1 \Sigma_{11}, U_2 \Sigma_{22}, ..., U_d \Sigma_{dd}) (V_1, V_2, ..., V_d)^\star \\
			&= \sum_{i=1}^{d} \Sigma_{ii} U_i V_i^\star
\end{align*}
such that
\begin{align*}
	\mat{X} &= \mat{X}_1 + \mat{X}_2 + ... + \mat{X}_d \\
	\mat{X}_i &= \sqrt{\lambda_i}U_iV_i^\star.
\end{align*}
The $(\sqrt{\lambda_i}, U_i, V_i)$ quantities are the \emph{eigentriples} of the SVD of $\mat{x}$. Sometimes $\sqrt{\lambda_i}V_i$ are called the \emph{principle components} of $\mat{X}$. $\mat{X}_i$'s are called the \emph{elementary matrices}.

Thirdly, group the eigentriples. This step is kinda optional, you can just use the elementary matrices directly, called \emph{elementary grouping}, but inspecting the eigentriples and grouping together those whose singular vectors vary at a similar frequency can reveal trends and periodic components to the data. 

In maths terms, partition the indices $\lbrace 1, 2, ..., d\rbrace$ into $l$ disjoint subsets $I_1, I_2, ..., I_l$, with $I = \lbrace 1,2,...,p \rbrace$. Therefore $\mat{X}_I$ corresponding to group $I$ is $\mat{X}_I = \mat{X}_{i_1} + \mat{X}_{i_2} + ... + \mat{X}_{i_p}$, and
\[
	\mat{X} = \mat{X}_{I_1} + \mat{X}_{I_2} + ... + \mat{X}_{I_l}
\]

Fourth, diagonally average the grouped matrices and extract the associated time-series. Each $\mat{X}_{I_j}$ matrix is \emph{Hankelised} (i.e., their anti-diagonals are averaged), and the time-series associated with hankelised matrix $\tilde{\mat{X}}_{I_j}$ is extracted by takling the main diagonal to get a reconstructed time series
\[
	\tilde{\mat{\mathbb{X}}}^{(j)} = (\tilde{x}_1^{(1)}, \tilde{x}_2^{(2)}, ...,\tilde{x}_N^{(N)}). 
\]
Therefore, the initial series $\mat{\mathbb{X}} = (x_1, x_2, ..., x_n)$ is decomposed into a sum of $l$ reconstructed subseries
\begin{align*}
	x_n &= \sum_j^{l} \tilde{x}_n^{(j)}, \quad \textrm{where} \quad n \in (1,2,...,N) \\
	\textrm{such that}& \\
	\mat{\mathbb{X}} &= \tilde{\mat{\mathbb{X}}}^{(1)} + \tilde{\mat{\mathbb{X}}}^{(2)} + ... + \tilde{\mat{\mathbb{X}}}^{(l)}.
\end{align*}

\begin{aside}
	Hankelisation is the process of taking some matrix $\mat{A}$ and replacing each anti-diagonal with the mean of the elements on that diagonal. E.g.
	\begin{align*}
		\mat{A} = \begin{pmatrix}
			a_1 & a_2 & a_3 \\
			a_4 & a_5 & a_6 \\
			a_7 & a_8 & a_9 
		\end{pmatrix} 
		\quad \rightarrow \quad	
		\tilde{\mat{A}} &= \begin{pmatrix}
			\tilde{a_1} & \tilde{a_2} & \tilde{a_3} \\
			\tilde{a_2} & \tilde{a_3} & \tilde{a_4} \\
			\tilde{a_3} & \tilde{a_4} & \tilde{a_5}
		\end{pmatrix}.
	\end{align*}


	In general for an $m \times n$ matrix,
	\begin{align*}
		\tilde{a_k} =& \frac{ \sum\limits_{(i,j) \in D_k} a_{ij} }{\textrm{Count}(D_k)} \\
		D_k =& \lbrace (i,j) : i+j = k+1, 1 \leq i \leq m, 1 \leq j \leq n \rbrace \\
		\textrm{Count}(D_k) \coloneqq & \textrm{The number of elements in $D_k$}
	\end{align*}
\end{aside}

To summarise, window length $L$ determins the resolution of the SSA, larger $L$ captures more data and therefore leads to better separability of different components. Also, $L$ defines the largest periodic features that can be captured. Trends are found by grouping eigentriples with slowly varying eigenvalues, periodicity is found by grouping similarly sized eigenvalues with periodically varying eigenvectors, noise is generally small (but not always) eigenvalues with stochastically varying eigenvectors.

Applying SSA to an image instead of a time series uses 2D-SSA \cite{Golyandina13}. In image space, instead of trends over time, you are looking at variation over space. The $\tilde{\mat{\mathbb{X}}}^{(j)}$ decomposed images therefore show decomposed spatial trends, periodicity, and noise.

In the SSA filtering routine, I've implicity assumed that noise is separated into components with smaller eigenvalues. For each decomposed image, $\tilde{\mat{\mathbb{X}}}^{(j)}$, the cumulative distribution function $p(\tilde{x}_i^{(j)} \leq x; \tilde{x}_i^{(j)} \in \tilde{\mathbb{X}}^{(j)}) $ is calculated, and then transformed to give each pixel a score $s_i^{(j)}$ via
\[
	s_i^{(j)} = 4(p(\tilde{x}_i^{(j)} \leq x; \tilde{x}_i^{(j)} \in \tilde{\mathbb{X}}^{(j)}) - 0.5)^2,
\]
which is intended to capture the squared distance away a pixel is from the mean value of its component image.

As low-eigenvalue components make up the majority of the noise, and the original image is the sum of the decomposed SSA component images. It stands to reason that any pixels that are consistently far from the median of the low-eigenvalue components will be pixels with a large component of noise. Therefore the pixels which, when averaged over all low-eigenvalue components, have a large score should be those that are most influenced by noise. So,
\[
	s_i = \sum\limits{j=l_1}{l_2} s_i^{(j)},
\]
where $l_1$, $l_2$ are the component images included in this analysis ($l_1=3$ and $l_2=12$ works well for a $4 \times 4$ window) and $0 \leq s_i \leq 1$. We set some fraction $s_f$ that is the cutoff of pixels we consider ``noisy" (I've found $s_f=0.95$ to be a good value).
\begin{align*}
	I_{\textrm{noisy}} = \lbrace i \textrm{  where  } s_i > s_f \rbrace
\end{align*}

The indices of $I_{\textrm{noisy}}$ can then be interpolated over to remove those noise artifacts and enable faster and more accurate deconvolution.







































