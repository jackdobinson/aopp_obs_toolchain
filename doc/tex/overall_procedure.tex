\begin{enumerate}

	\item Collate data.
	\begin{itemize}
		\item Observation data
		\item PSF data
	\end{itemize}
	
	\item Normalise and pre-process data.
	\begin{itemize}
		\item Create PSF model (if needed)
		\item Normalise PSF, ensure that it
			\begin{itemize}
				\item sums to 1
				\item has an odd number of rows/columns
				\item is centered, usually the brightest pixel is ok, but sometimes it may have to be more exact.
				\item contains no NANs, INFs, or negative values. Try interpolating across them if needed.
			\end{itemize}
		\item Pre-process observation data: interpolate over NANs and INFs, use SSA filtering to remove artifacts if needed, remove particularly bad artifacts by hand if needed.
		\item Trim down "extra space" around observation and PSF. The fewer pixels an image has, the faster the algorithm will run.
	\end{itemize}

	\item \label{itm:set_deconv_params} Deterimine deconvolution parameters, this usually involves educated guesses and a bit of iteration.
	
	\item Inspect results and goto step \ref{itm:set_deconv_params} to tweak parameters. Generally want to tweak them even if results are good, as you want to know how delicate the solution is. Some general classes of problems and their possible cause are:
	\begin{description}
		\item{\hspace{-1em}Speckling\\} You generally want to use one of the variable threshold modes. Try setting 0 $>$ \emph{threshold} $>$ -1, \emph{threshold}=-1, or \emph{threshold}=-2.
		\item{\hspace{-1em}Very uncorrelated and/or a stripe accross the field\\} Usually due to a non-centered PSF i.e., the subtractions from the algorithm are not correlated with the pixels it is selecting.
		\item{\hspace{-1em}Large areas of Zeros\\} Normally means that you have to few iterations, try increasing it or try increasing the \emph{loop gain}. Increasing the \emph{loop gain} technically makes the algorithm less accurate, but speeds it up by the factor it was increased by.
		\item{\hspace{-1em}Very persistent problems\\} In the case you need a nuclear option, you can try smoothing the components with a gaussian (usually around the size of the core of the PSF). This has the effect of regularising the over-fitted components, removing the high-frequency noise. This is what the traditional \textsc{clean} algorithm does to create the \emph{clean map}, but so far for us the components have not been troublesome to use by themselves.
	\item{\hspace{-1em}Trouble getting flux/irradiance constant\\} If the total flux/irradiance of the image is varying wildly, consider adding the residual map to the components (or to the \emph{clean map} if you need to). Usually the difference is very small, but sometimes there's information you can't ingore in the residual.
		 
	\end{description}

\end{enumerate} 
