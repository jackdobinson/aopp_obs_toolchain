#!/usr/bin/env python3
"""
Creates NEMESIS compatible *.spx files from a set of target fits cubes and NEMESIS run templates.

The `if __name__=='__main__':` statement allows execution of code if the script is called directly.
eveything else not in that block will be executed when a script is imported. 
Import statements that the rest of the code relies upon should not be in the if statement, python
is quite clever and will only import a given package once, but will give it multiple names if it
has been imported under different names.

Standard library documentation can be found at https://docs.python.org/3/library/

Packages used in this program are:
	sys
	os 
"""

# TODO:
# import standard library packages
import sys # https://docs.python.org/3/library/sys.html
import os # https://docs.python.org/3/library/os.html
import stat
import datetime as dt
import subprocess as sp
# import 3rd party packages
from astropy.io import fits
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as ptc
# import my packages
import fitscube.fit_region
import fitscube.process.sinfoni
import fitscube.exceptions
import subsample
import utils as ut # used for convenience functions
#import fitscube_fit_region
import nemesis.write
import nemesis.read
import slurm
import fitscube.limb_darkening_synthetic_img
import nemesis.copy_inputs

def main(argv):
	"""This code will be executed if the script is called directly"""
	args = parse_args(argv)
	print(ut.str_wrap_in_tag(ut.str_dict(args), 'ARGUMENTS'))
	
	# get a unique name for each nemesis template
	nemesis_template_unique_path_part = get_unique_path_part([os.path.abspath(x) for x in args['nemesis_templates']])
	print(nemesis_template_unique_path_part)

	# get a unique name for each target cube
	tc_unique_path_part = get_unique_path_part([os.path.abspath(os.path.dirname(x)) for x in args['target_cubes']])
	print(tc_unique_path_part)

	# create a holder for submission scripts
	slurm_submission_scripts = []
	
	for i, tc in enumerate(args['target_cubes']):
		tc_dir = os.path.dirname(tc)

		# create directory tree for spx files
		ut.pINFO('Operating on target cube "{}"'.format(tc_unique_path_part[i]))
		
		# assume we have a cube that conforms to my format
		with fits.open(tc) as hdul:
			#hdul = fits.open(tc)
			wavgrid = fitscube.process.sinfoni.datacube_wavelength_grid(hdul[0])
			
			# scale error if we have to, should be separate from pre-processing error
			hdul = scale_error(hdul, args['data_err_scaling'], show_plots=False)

			# mask out bright spots (clouds)
			if args['cloud_mask.enable']:
				if args['cloud_mask.load'] is not None:
					mask_fits_file = os.path.join(tc_dir, args['cloud_mask.load'])
					ut.pINFO(f'Loading maks from file "{mask_fits_file}"')
					with fits.open(mask_fits_file) as mask_hdul:
						hdul[0].data = np.ma.array(hdul[0].data, mask=mask_hdul[0].data, fill_value=np.nan)
				else:
					hdul[0].data = fitscube.limb_darkening_synthetic_img.mask_bright_spots(hdul, 
										wavgrid=wavgrid, mask_type=args['cloud_mask.mode'],
										show_plot=args['show_plots'], show_animated_plot=False, 
										save_plot=False, save_animated_plot=False, 
										plot_dir=None, frame_dir=None, overwrite=True)
				if args['cloud_mask.edit']:
					viewer = fitscube.fit_region.FitscubeMaskPainter(hdul, mask=hdul[0].data.mask)
					viewer.run()
					hdul[0].data = viewer.getMaskedData()

				if args['cloud_mask.save'] is not None:
					mask_hdul = fits.HDUList([fits.PrimaryHDU(hdul[0].data.mask)])
					mask_hdul.writeto(os.path.join([tc_dir, args['cloud_mask.save']]))
			else:
				hdul[0].data = np.ma.array(hdul[0].data, mask=np.zeros_like(hdul[0].data), fill_value=np.nan) # create an empty mask
			
			
			# get data, this is where the majority of the difficult stuff happens, the rest of the code in main is mostly just arranging the data
			# we get from here.
			#(containing_folders, nemesis_datas) = get_limb_darkening_data(tc, hdul, wavgrid, args)
			(containing_folders, nemesis_datas) = create_nemesis_run_ensemble(tc, hdul, args['wavgrid'], args)

		# Loop over the nemesis templates to apply to the data we have chosen
		for j, nemesis_template in enumerate(args['nemesis_templates']):
			ut.pINFO('Using nemesis template "{}"'.format(nemesis_template))
		
			# for each nemesis template and each target cube, create a unique subdirectory where we can store runs
			tc_top_slurm_dir = os.path.abspath(	os.path.join(	args['slurm_dir'], 
																nemesis_template_unique_path_part[j], 
																tc_unique_path_part[i])
												)
		
			# create a holder for job scripts
			slurm_job_scripts = []
		
			# for each set of data generated by the target cube
			for containing_folder, nemesis_data in zip(containing_folders, nemesis_datas):
				# create directory tree
				current_slurm_dir = os.path.join(tc_top_slurm_dir, containing_folder)
				os.makedirs(current_slurm_dir, exist_ok=True)
				
				# copy template data
				#copy_command = [sys.executable, args['copy_nemesis_inputs_path'], nemesis_template, 
				#				current_slurm_dir, '--from_run_name', 'neptune', '--to_run_name', args['runname'], 
				#				'--overwrite', 'no']
				#ut.pINFO(' '.join(copy_command))
				#sp.run(copy_command)	
				argl = [	nemesis_template, 
							current_slurm_dir, 
							'--src.runname', args['copy.src.runname'], 
							'--dest.runname', args['runname'],
							'--src.n_search_parents', args['copy.src.n_search_parents'], 
							'--nemesis.tags', *args['copy.nemesis.tags'], 
							]
				if args['copy.dry_run']:
					argl.append('--dry_run')
				if not args['copy.dest.overwrite']:
					argl.append('dest.no_overwrite')
				nemesis.copy_inputs.main(list(map(str, argl)))
							
				# overwrite copied template data with our new data	
				write_nemesis_data(nemesis_data, os.path.join(current_slurm_dir, args['runname']))
				
				# create slurm submission scripts in each directory
				current_slurm_job_script = slurm.create_job_script(current_slurm_dir)
				slurm_job_scripts.append(current_slurm_job_script)
				
				# change permissions of job script to give read,write,execute permission to user,
				# read permission to group and other
				os.chmod(current_slurm_job_script, stat.S_IRWXU|stat.S_IRGRP|stat.S_IROTH)
					
			
			# create master submission script at top level of directory
			tc_slurm_submission_script = slurm.create_submission_script(tc_top_slurm_dir, slurm_job_scripts)
			slurm_submission_scripts.append(tc_slurm_submission_script)
			os.chmod(tc_slurm_submission_script, stat.S_IRWXU|stat.S_IRGRP|stat.S_IROTH)
			
	for sss in slurm_submission_scripts:
		ut.pINFO('Slurm Submission Script created at: {}'.format(sss))
	gss = slurm.create_global_script(args['slurm_dir'], slurm_submission_scripts)
	ut.pINFO('Global slurm submission script created at: {}'.format(gss))
	os.chmod(gss, stat.S_IRWXU|stat.S_IRGRP|stat.S_IROTH)

	gsocs = slurm.create_global_output_clean_script(args['slurm_dir'])
	ut.pINFO('Global slurm clean output script created at: {}'.format(gsocs))
	os.chmod(gsocs, stat.S_IRWXU|stat.S_IRGRP|stat.S_IROTH)

	notesf = create_notes_file(args['slurm_dir'], args)
	ut.pINFO('Notes file created at: {}'.format(notesf))
		
	ut.pIMPORT('IF RUNS ARE FAILING TRY TO RECOMPILE NEMESIS ON THIS MACHINE')

	return

def write_nemesis_data(nemesis_data, runname):
	"""
	Takes a dictionary consisting of key-value pairs that describe a file type (keys) and the data to write to them
	(values). 
	"""
	rundir = os.path.dirname(runname)
	if 'spx' in nemesis_data:
		nemesis.write.spx(nemesis_data['spx'], runname=runname)
	if 'set' in nemesis_data:
		nemesis.write.set(nemesis_data['set'], runname=runname)


def create_notes_file(topdir, input_args):
	notesf = os.path.join(topdir, 'notes.txt')
	with open(notesf, 'w') as f:
		f.write('This file keeps a record of the command that created this directory structure.\n')
		f.write('Script "{}"\n'.format(__file__))
		f.write('Called at {}\n'.format(dt.datetime.now()))
		f.write('Called using arguments\n')
		f.write(ut.str_wrap_in_tag(ut.str_dict(input_args), 'ARGUMENTS'))
		f.write('\n') # ut.str_wrap_in_tag does not include a final newline character
		f.write('\n')
	return(notesf)

def gauss_lobatto_quadrature(n, interval_min, interval_max):
	"""
	Gets the points and weights for gauss-lobatto quadrature of n points in an interval.
	"""
	leg_series_coeffs = np.zeros(n) # goes up to the n-1^th polynomial, currently this is a series of legendre 
									# polynomials with all coefficients = 0 (e.g. 0*L_0 + 0*L_1 + ... + 0*L_(n-1) )
	leg_series_coeffs[n-1] = 1 # make the n-1^th polynomial in the series have coefficient = 1
	# e.g. 0*L_0 + 0*L_1 + ... + 1*L_(n-1)

	# differentiate the legendre polynomial series defined above
	diff_leg_series_coeffs = np.polynomial.legendre.legder(leg_series_coeffs, m=1)

	# the n quadrature points (x_i's) of gauss-lobatto are the roots of the deriviative of 
	# the n-1^th Legendre polynomial for the interstital points, and -1, 1 for the endpoints
	l_roots = np.polynomial.legendre.legroots(diff_leg_series_coeffs)

	quad_points = np.zeros(n)
	quad_points[0] = -1
	quad_points[-1] = 1
	quad_points[1:-1] = l_roots

	# the interstitial weights of the quadrature points are given by
	# w_i = 2/(n*(n-1)*(L_(n-1)(x_i))^2), where i = 1 -> (n-1)
	interstitial_weights = 2/(n*(n-1)*(np.polynomial.legendre.legval(l_roots, leg_series_coeffs))**2)
	# the weights of the endpoints are given by
	# w_0 = w_n = 2/(n*(n-1))
	endpoint_weights = [2/(n*(n-1))]*2
	
	weights = np.zeros(n)
	weights[0] = endpoint_weights[0]
	weights[-1] = endpoint_weights[1]
	weights[1:-1] = interstitial_weights
	
	# gauss-lobatto quadrature is defined on interval [-1,1] so have to adjust for desired interval
	interval_range = interval_max - interval_min
	#quad_points += 1
	#quad_points /=2
	#quad_points += (interval_min)
	#quad_points *= (interval_range)
	
	quad_points *= interval_range/2
	quad_points += (interval_min + interval_max)/2
	
	return(quad_points, weights)

def gauss_quadrature(n, interval_min, interval_max):
	"""
	Get the quadrature points and weights for gaussian quadrature for some interval.
	"""
	q_points, weights = np.polynomial.legendre.leggauss(n)
	q_points *= (interval_max - interval_min)/2
	q_points += (interval_max + interval_min)/2
	return(q_points, weights)

def create_nemesis_run_ensemble(tc, hdul, wavgrid, args):
	"""
	Creates directory tree names and list of dictonaries that contain nemesis input file data
	for each latitude swath where the limb-darkening is being calculated, calls a function that does
	the actual computation.

	# ARGUMENTS #
		tc
			Target fits cube that is being operated on
		hdul
			Header Data Unit List of the fits cube that is being operated on
		wavgrid
			Wavelength points present in the fits cube being operated on
		args
			Arguments passed to the program (see 'parse_args()')
	
	# RETURNS #
		containing_folders
			Paths of folders to create (relative to the top nemesis run folder), these are
			usually named using a unique identifier that relates to (in this case) the latitude
			swath included in the sub-run (e.g. "./lat_-10.0_to_-5.0").
		nemesis_datas
			List of dictionaries of data need to create various nemesis input files, there should
			be one dictionary for each 'containing_folder' that contains all the data needed to create
			the input files for the sub-run in that specific folder.		
	"""
	
	if not (args['limb_darkening_type'] is None):
		if args['limb_darkening_type'] == 'simple':
			return(get_limb_darkening_data(tc, hdul, wavgrid, args))
		elif args['limb_darkening_type'] == 'minnaert':
			return(get_minnaert_limb_darkening_data(tc, hdul, wavgrid, args))
		else:
			ut.pERROR('The limb darkening type {} has not been implemented'.format(args['limb_darkening_type']))
			raise fitscube.exceptions.FitscubeNotImplementedError

	if args['region.pixel_box'] is not None:
		# we have a box of pixels to average together
		if args['region.as_set']:
			return(get_pixel_box_set_data(tc, hdul, wavgrid, args))
		else:
			return(get_pixel_box_average_data(tc, hdul, wavgrid, args))
	else:
		print(f'ERROR: No ensemble type passed to create data from, exiting...')
		raise fitscube.exceptions.FitscubeNotImplementedError

	return([],[]) # if we somehow get here, return empty containers

def get_pixel_box_average_data(tc, hdul, wavgrid, args):
	"""
	Get a pixel box from the passed hdul and average it's contents, create appropriate *.spx file
	"""
	print('INFO: In "get_pixel_box_average_data()"')

	px_minx, px_miny, px_maxx, px_maxy = args['region.pixel_box']
	px_num = (px_maxx-px_minx) * (px_maxy-px_miny)
	aslice = np.s_[px_miny:px_maxy,px_minx:px_maxx] # image plane slice to use

	print(f'INFO: aslice {aslice}')
	
	hdul_wavgrid = fitscube.process.sinfoni.datacube_wavelength_grid(hdul[0])
	if wavgrid is None:
		wavgrid = hdul_wavgrid
	if args['telluric_error_scaling']:
		# load this if we need it later
		opac_data = np.loadtxt(args['telluric_reference_file'])
		opac_data_interp = np.interp(wavgrid, opac_data[:,0], opac_data[:,1])			
	
	spec_flux_slice = hdul[0].data[:, aslice[0], aslice[1]]
	spec_flux_slice_mean = np.nanmean(spec_flux_slice, axis=(1,2))
	spec_flux = np.interp(wavgrid, hdul_wavgrid, spec_flux_slice_mean)

	spec_err_slice = hdul['ERROR'].data[:, aslice[0], aslice[1]]
	spec_err_slice_mean = np.nanmean(spec_err_slice, axis=(1,2))
	spec_err = np.interp(wavgrid, hdul_wavgrid, spec_err_slice_mean)

	# reduce error by sqrt(pix_num)
	spex_err = spec_err/np.sqrt(px_num)

	print(f'INFO: spec_flux_slice_mean {spec_flux_slice_mean}')
	print(f'INFO: spec_err_slice_mean {spec_err_slice_mean}')

	lats = hdul['LATITUDE'].data[aslice]
	lons = hdul['LONGITUDE'].data[aslice]
	zens = hdul['ZENITH'].data[aslice]
	weights = hdul['DISK_FRAC'].data[aslice]
	print(f'INFO: lats {lats}')
	print(f'INFO: lons {lons}')
	print(f'INFO: zens {zens}')
	print(f'INFO: weights {weights}')
	meanlat = np.nanmean(lats)
	meanlon = np.nanmean(lons)
	ngeom = 1
	nconvs = [len(wavgrid)]
	navs = [lats.flatten().shape[0]]
	fov_averaging_record = []

	flats = lats.flatten()
	flons = lons.flatten()
	fzens = zens.flatten()
	fweights = weights.flatten()
	for i in range(ngeom):
		a_fov = []
		for j in range(navs[i]):
			a_fov.append([flats[j], flons[j],fzens[j],fzens[j],180.0,fweights[j]])
		fov_averaging_record.append(a_fov)
	spec_record = [np.zeros((nconv,3)) for nconv in nconvs]
	for i in range(ngeom):
		spec_record[i][:,0] = wavgrid
		spec_record[i][:,1] = spec_flux
		spec_record[i][:,2] = spec_err
		if args['telluric_error_scaling']:
			spec_record[i][:,2] *= 1.0/opac_data_interp
		# make sure error is at least some prescribed fraction of the signal
		base_frac_err = args['base_fractional_error']*spec_record[i][:,1]
		spec_record[i][:,2] = np.where(spec_record[i][:,2] > base_frac_err, spec_record[i][:,2], base_frac_err)
		# cap maximum error at 10x the value
		spec_record[i][:,2] = np.where(spec_record[i][:,2] > 10*spec_record[i][:,1], 10*spec_record[i][:,1], spec_record[i][:,2])
				

	# construct *.spx data
	spx_data = {
		'fwhm':args['spec_fwhm'],
		'latitude':meanlat,
		'longitude':meanlon,
		'ngeom':ngeom,
		'nconvs':nconvs,
		'navs':navs,
		'fov_averaging_record':fov_averaging_record,
		'spec_record':spec_record
		}

	#print(spx_data)
	containing_folders = [f'./lat_{meanlat:+#07.2f}_lon_{meanlon:+#07.2f}']
	nemesis_datas=[{'spx':spx_data}]
	return(containing_folders, nemesis_datas)

def get_pixel_box_set_data(tc, hdul, wavgrid, args):
	"""
	Uses a pixel box to create a set of runs, one per pixel
	"""
	print('INFO: In "get_pixel_box_set_data()"')

	px_minx, px_miny, px_maxx, px_maxy = args['region.pixel_box']
	px_num = (px_maxx-px_minx) * (px_maxy-px_miny)
	aidx = np.mgrid[px_miny:px_maxy,px_minx:px_maxx] # image plane slice to use

	print(f'INFO: aidx {aidx}')
	
	hdul_wavgrid = fitscube.process.sinfoni.datacube_wavelength_grid(hdul[0])
	if wavgrid is None:
		wavgrid = hdul_wavgrid
	if args['telluric_error_scaling']:
		# load this if we need it later
		opac_data = np.loadtxt(args['telluric_reference_file'])
		opac_data_interp = np.interp(wavgrid, opac_data[:,0], opac_data[:,1])			
	
	#spec_flux_slice = hdul[0].data[:, aslice[0], aslice[1]]
	#spec_err_slice = hdul['ERROR'].data[:, aslice[0], aslice[1]]
	#lats = hdul['LATITUDE'].data[aslice]
	#lons = hdul['LONGITUDE'].data[aslice]
	#zens = hdul['ZENITH'].data[aslice]
	#weights = hdul['DISK_FRAC'].data[aslice]

	#nz,ny,nx = spec_flux_slice.shape
	#print(spec_flux_slice.shape)
	#print(spec_flux_slice.reshape(nz,ny*nx).shape)

	nemesis_datas = []	
	containing_folders = []
	yy, xx = aidx
	for y, x in zip(yy.flatten(), xx.flatten()):
		print(y,x)
		spec_flux_raw = hdul[0].data[:,y,x]
		print(hdul[0].data.shape)
		print(spec_flux_raw.shape)
		spec_flux = np.interp(wavgrid, hdul_wavgrid, hdul[0].data[:, y, x])
		spec_err = np.interp(wavgrid, hdul_wavgrid, hdul['ERROR'].data[:, y, x])
		lat = hdul['LATITUDE'].data[y,x]
		lon = hdul['LONGITUDE'].data[y,x]
		zen = hdul['ZENITH'].data[y,x]
		weight = hdul['DISK_FRAC'].data[y,x]

		ngeom = 1
		nconvs = [len(wavgrid)]
		navs = [1]
		fov_averaging_record = []

		for i in range(ngeom):
			a_fov = []
			for j in range(navs[i]):
				a_fov.append([lat, lon,zen,zen,180.0,weight])
			fov_averaging_record.append(a_fov)
		spec_record = [np.zeros((nconv,3)) for nconv in nconvs]
		for i in range(ngeom):
			spec_record[i][:,0] = wavgrid
			spec_record[i][:,1] = spec_flux
			spec_record[i][:,2] = spec_err
			if args['telluric_error_scaling']:
				spec_record[i][:,2] *= 1.0/opac_data_interp
			# make sure error is at least some prescribed fraction of the signal
			base_frac_err = args['base_fractional_error']*spec_record[i][:,1]
			spec_record[i][:,2] = np.where(spec_record[i][:,2] > base_frac_err, spec_record[i][:,2], base_frac_err)
			# cap maximum error at 10x the value
			spec_record[i][:,2] = np.where(spec_record[i][:,2] > 10*spec_record[i][:,1], 10*spec_record[i][:,1], spec_record[i][:,2])
					

		# construct *.spx data
		spx_data = {
			'fwhm':args['spec_fwhm'],
			'latitude':lat,
			'longitude':lon,
			'ngeom':ngeom,
			'nconvs':nconvs,
			'navs':navs,
			'fov_averaging_record':fov_averaging_record,
			'spec_record':spec_record
			}
		nemesis_datas.append({'spx':spx_data})
		containing_folders.append(f'./lat_{lat:+#07.2f}_lon_{lon:+#07.2f}')

	return(containing_folders, nemesis_datas)


def get_minnaert_limb_darkening_data(tc, hdul, wavgrid, args):
	"""
	Using
                 k  k-1
	 I     /I\  u  u
	--- = |---|  0
	 F     \F/
             0
	Where
		u = cos(zenith)
		u_0 = cos(solar zenith)
		k, (I/F)_0 are fixed parameters
		I/F is the flux at the zenith angle we are interested in

	In out case, the solar zenith angle and the zenith angle are approximately the same as the earth and sun are
	very close when seen from Neptune

	or in logs
		ln[u(I/F)] = ln[(I/F)_0] + k ln[u_0 u]
	"""
	if args['limb_darkening.full_disk']:
		lat_limits = [(-90,90)]
	else:
		lat_range = args['latitude.max'] - args['latitude.min']
		latvals = np.linspace(args['latitude.min'], args['latitude.max'], args['latitude.n']+1)
		if (args['latitude.bin_step'] is None) and (args['latitude.bin_width'] is None):
			if args['latitude.micro_step']:
				lat_limits = np.zeros((args['latitude.n']-1,2))
				print(latvals)
				lat_limits[:,0] = latvals[:-2]
				lat_limits[:,1] = latvals[2:]
			else:
				lat_limits = np.zeros((args['latitude.n'],2))
				lat_limits[:,0] = latvals[:-1] 
				lat_limits[:,1] = latvals[1:]
		elif args['latitude.bin_width'] is None:
			lat_delta = (args['latitude.n']*args['latitude.bin_width'] - lat_range)/(args['latitude.n']-1)
			lat_limits[:,0] = np.linspace(args['latitude.min'], args['latitude.max']-args['latitude.bin_width'], args['latitude.n'])
			lat_limits[:,1] = np.linspace(args['latitde.min']+args['latitude.bin_width'], args['latitde.max'], args['latitude.n'])
		elif args['latitude.bin_step'] is None:
			lat_width = lat_range - args['latitude.n']*args['latitude.bin_step']
			lat_limits[0,:] = np.linspace(args['latitude.min'], args['latitude.max']-lat_width, args['latitude.n'])
			lat_limits[1,:] = np.linspace(args['latitude.min']+args['latitude.bin_step'], args['latitude.max'], args['latitude.n'])
		else:
			# both width and step are given so we would overwite something
			print('ERROR: both "--latitude.bin_width" and "--latitude.bin_step" are given. Problem is over-specified. Exiting...')
			sys.exit()

	if args['limb_darkening.max_zen'] is not None:
		zenmax = args['limb_darkening.max_zen']

	mld_fulls, mld_smalls = [], []
	for lat_min, lat_max in lat_limits:
		mld_full, mld_small = fitscube.limb_darkening_synthetic_img.minnaert_limb_darkening(hdul, check_region_flag=False, zenmax=zenmax, latmin=lat_min, latmax=lat_max, 
																							show_plot=False, show_animated_plot=False, save_plot=False, save_animated_plot=False)
		mld_fulls.append(mld_full)
		mld_smalls.append(mld_small)
	# mld's contain minnaert limb darkening coefficients for each wavelength, and we have a set of them that cover a group of latitude slices.
	# need to make a folder name for each latitude slice and a *.spx file for each latitude slice using the information in the mld's

	if args['limb_darkening.max_zen']:
		mld_fulls = mld_smalls # use the restricted zenith angle parameters

	containing_folders = []
	nemesis_datas = []
	output_zen_angles = [0, 60]


	for mld_full in mld_fulls:
		containing_folder = './lat_{:+07.2f}_to_{:+07.2f}'.format(mld_full['latmin'], mld_full['latmax'])
		if wavgrid is None:
			outwavgrid = mld_full['wavgrid']
		else:
			outwavgrid = wavgrid
		spectral_flux_at_zen = np.zeros((len(outwavgrid),len(output_zen_angles)))
		spectral_err_at_zen = np.zeros((len(outwavgrid), len(output_zen_angles)))
		for j, za in enumerate(output_zen_angles):
			mld_wavgrid = mld_full['wavgrid']
			log_IperF0s = mld_full['log_IperF0s']
			IperF0s = np.exp(log_IperF0s)
			print(np.nanmin(IperF0s), np.nanmax(IperF0s), np.nanmedian(IperF0s))
			u = np.cos(za*np.pi/180)
			u0 = np.cos(za*np.pi/180)
			ks = mld_full['ks']
			print(np.min(ks), np.max(ks), np.median(ks))
			IperF = IperF0s*(u0**ks)*(u**(ks-1))
			print(np.min(IperF), np.max(IperF), np.median(IperF))
			spectral_flux_at_zen[:,j] = np.nan_to_num(np.interp(outwavgrid, 
														mld_full['wavgrid'], 
														IperF
														))
			# TODO: May need to do something different for the errors on the spectral flux, they seem small, maybe just do 10%?
			a, b = np.array([np.sum(_x) for _x in mld_full['residuals']]).flatten(), np.array([sum(_x)/len(_x) if len(_x)!=0 else np.nan for _x in mld_full['log_uIperFs']]).flatten()
			N = int(len(outwavgrid)/2)
			ut.pINFO(f'residuals.shape {a.shape} residuals[{N}] {a[N]} {type(a[N])}')
			ut.pINFO(f'log_uIperFs.shape {b.shape} log_uIperFs[N] {b[N]}')
			ut.pINFO(f'spectral_flux_at_zen[:,{j}].shape {spectral_flux_at_zen[:,j].shape} spectral_flux_at_zen[{N}, {j}] {spectral_flux_at_zen[N, j]}')
			c = a/(b*np.sqrt(mld_full['npx']))
			ut.pINFO(f'c.shape {c.shape}')
			ut.pINFO(f'c[{N}] {c[N]} {type(c[N])}')
			#print(mld_full['log_IperF0s'])
			print(c)
			spectral_err_at_zen[:,j] = np.interp(outwavgrid, mld_full['wavgrid'], c)*spectral_flux_at_zen[:,j]


		if args['telluric_error_scaling']:
			# load this if we need it later
			opac_data = np.loadtxt(args['telluric_reference_file'])
			opac_data_interp = np.interp(outwavgrid, opac_data[:,0], opac_data[:,1])			
			
		# spx data
		spec_fwhm = args['spec_fwhm']
		mean_lat = 0.5*(mld_full['latmin']+mld_full['latmax'])
		mean_lon = 0
		ngeom = len(output_zen_angles)
		navs = [1]*ngeom
		nconvs = [len(outwavgrid)]*ngeom
		fov_averaging_record = []
		spec_record = []
		for j, za in enumerate(output_zen_angles):
			fov_each_geom = []
			for k in range(navs[j]):
				fov = (
					mean_lat,
					mean_lon,
					za, # solar zenith angle same as observer zenith angle if target is neptune
					za,
					180, # sun and earth are so close to eachother from Neptune's point of view the light is effectively backscattered
					1.0
				)
				fov_each_geom.append(fov)
			fov_averaging_record.append(fov_each_geom)
			spec_data = np.zeros((nconvs[j],3))
			spec_data[:,0] = outwavgrid
			spec_data[:,1] = spectral_flux_at_zen[:,j]
			spec_data[:,2] = np.abs(spectral_err_at_zen[:,j]) # error should be +ve
			if args['telluric_error_scaling']:
				spec_data[:,2] *= 1.0/opac_data_interp
			# make sure error is at least some prescribed fraction of the signal
			base_frac_err = args['base_fractional_error']*spec_data[:,1]
			spec_data[:,2] = np.where(spec_data[:,2] > base_frac_err, spec_data[:,2], base_frac_err)
			# cap maximum error at 10x the value
			spec_data[:,2] = np.where(spec_data[:,2] > 10*spec_data[:,1], 10*spec_data[:,1], spec_data[:,2])
				
			spec_record.append(spec_data)
		
		spx_dict = {	'fwhm':spec_fwhm,
						'latitude':mean_lat,
						'longitude':mean_lon,
						'ngeom':ngeom,
						'nconvs':nconvs,
						'navs':navs,
						'fov_averaging_record':fov_averaging_record,
						'spec_record':spec_record}

		nemesis_datas.append({'spx':spx_dict})
		containing_folders.append(containing_folder)

	return(containing_folders, nemesis_datas)
	        
def get_limb_darkening_data(tc, hdul, wavgrid, args):
	"""
	Creates directory tree names and list of dictonaries that contain nemesis input file data
	for each latitude swath where the limb-darkening is being calculated

	# ARGUMENTS #
		tc
			Target fits cube that is being operated on
		hdul
			Header Data Unit List of the fits cube that is being operated on
		wavgrid
			Wavelength points present in the fits cube being operated on
		args
			Arguments passed to the program (see 'parse_args()')
	
	# RETURNS #
		containing_folders
			Paths of folders to create (relative to the top nemesis run folder), these are
			usually named using a unique identifier that relates to (in this case) the latitude
			swath included in the sub-run (e.g. "./lat_-10.0_to_-5.0").
		nemesis_datas
			List of dictionaries of data need to create various nemesis input files, there should
			be one dictionary for each 'containing_folder' that contains all the data needed to create
			the input files for the sub-run in that specific folder.		
	"""
	lat_region_picker = fitscube.fit_region.FitscubeLatitudeRegionPicker(hdul)
	lat_region_picker.run()
	ut.pINFO('Getting data for chosen region')
	#rgn_idxs = np.transpose(lat_region_picker.region_idxs())
	rgn_idxs = lat_region_picker.getRegionIdxs()
	print(f'rgn_idxs.shape {rgn_idxs.shape}')
	#print(rgn_idxs)
	rgn_spec = hdul[0].data[:,rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_spec.shape {rgn_spec.shape}')
	#print(rgn_spec)
	rgn_lats = hdul['LATITUDE'].data[rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_lats.shape {rgn_lats.shape}')
	#print(rgn_lats)
	rgn_lons = hdul['LONGITUDE'].data[rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_lons.shape {rgn_lons.shape}')
	#print(rgn_lons)
	rgn_zens = hdul['ZENITH'].data[rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_zens.shape {rgn_zens.shape}')
	#print(rgn_zens)
	rgn_frac = hdul['DISK_FRAC'].data[rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_frac.shape {rgn_frac.shape}')
	#print(rgn_frac)
	rgn_errs = hdul['ERROR'].data[:,rgn_idxs[0],rgn_idxs[1]]
	print(f'rgn_errs.shape {rgn_errs.shape}')
	#print(rgn_errs)	

	#f0,a0 = make_fig_axes(1,1,1)
	#img00 = a0[0].imshow(hdul[0].data[300,:,:], origin='lower')
	#sca00 = a0[0].scatter(rgn_idxs[0,:], rgn_idxs[1,:])
	#plt.show()


	rgn_latmean = np.nanmean(rgn_lats)
	rgn_spec /= rgn_frac[None,:]
	rgn_errs /= rgn_frac

	zen_std=5 # spread of zenith angles to include in weighting function
	print(zen_std)

	if args['set_file']:
		set_data = nemesis.read.set(args['set_file'])
		qcos = set_data['quad_points']
		qweights = set_data['quad_weights']
		nzens = len(qcos)
	else:
		nzens = args['n_zen_quadrature']
		# gauss-lobatto is symmetric, so we only need the +ve half
		# so when we have 5 zenith angles, we actually use 10 quadrature points
		# and only record the +ve half.
		qcos, qweights = gauss_lobatto_quadrature(2*nzens, -1, 1) 
		qcos = qcos[nzens:]
		qweights = qweights[nzens:]

	qzen = 180/np.pi*np.arccos(qcos)
	qrad = np.zeros((nzens, rgn_spec.shape[0]))
	qzen = np.flip(qzen)
	qrad_err = np.zeros_like(qrad)
	#print(qzen.shape)
	#print(qrad.shape)
	for i in range(nzens):
		weights = np.exp(-((rgn_zens-qzen[i])/zen_std)**2)
		#print(weights.shape)
		#print(weights)
		#print(rgn_spec.shape)
		w_sum = np.nansum(weights)
		qrad[i] = np.nansum(rgn_spec*weights[None,:], axis=(1))/w_sum
		qrad_err[i] = np.sqrt(np.nansum((rgn_errs**2)*weights[None,:],axis=(1)))/w_sum

	# unfolded version
	rgn_zens_uf = rgn_zens*np.sign(rgn_lons)
	qzen_uf = np.zeros((2*nzens-1))
	qzen_uf[:nzens] = 180/np.pi*np.arccos(qcos)
	qzen_uf[nzens:] = -180/np.pi*np.arccos(np.flip(qcos[:-1]))
	qrad_uf = np.zeros((2*nzens-1, rgn_spec.shape[0]))
	qrad_err_uf = np.zeros_like(qrad_uf)
	for i in range(2*nzens-1):
		weights = np.exp(-((rgn_zens_uf-qzen_uf[i])/zen_std)**2)
		#print(weights.shape)
		#print(weights)
		#print(rgn_spec.shape)
		w_sum = np.nansum(weights)
		qrad_uf[i] = np.nansum(rgn_spec*weights[None,:], axis=(1))/w_sum
		qrad_err_uf[i] = np.sqrt(np.nansum((rgn_errs**2)*weights[None,:],axis=(1)))/w_sum


	#print(np.nanmedian(rgn_spec, axis=0))
	#print(np.nanmedian(qrad_err, axis=1))
	#print(qzen)
	#print(np.nanmedian(qrad, axis=1))
	
	# INFO: qrad_err and qrad_err_uf seems to be too big by a large factor, why?

	# Plotting
	f1, a1 = make_fig_axes(2,1,2)
	a1[0].scatter(rgn_zens, np.nanmedian(rgn_spec, axis=0), color='tab:orange')
	a1[0].plot(qzen, np.nanmedian(qrad, axis=1))
	a1[0].fill_between(qzen, 
						np.nanmedian(qrad, axis=1) - np.nanmedian(qrad_err,axis=1), 
						np.nanmedian(qrad, axis=1) + np.nanmedian(qrad_err,axis=1), 
						alpha=0.5, color='tab:orange')
	a1[0].set_ylim(0, np.max(np.nanmedian(rgn_spec, axis=0)))
	a1[0].set_title('Zenith angles')

	a1[1].scatter(rgn_zens_uf, np.nanmedian(rgn_spec, axis=0), color='tab:orange')
	a1[1].plot(qzen_uf, np.nanmedian(qrad_uf, axis=1))
	a1[1].fill_between(qzen_uf, 
						np.nanmedian(qrad_uf, axis=1) - np.nanmedian(qrad_err_uf,axis=1), 
						np.nanmedian(qrad_uf, axis=1) + np.nanmedian(qrad_err_uf,axis=1), 
						alpha=0.5, color='tab:orange')
	a1[1].set_ylim(0, np.max(np.nanmedian(rgn_spec, axis=0)))
	a1[1].set_title('Zenith angles unfolded')

	plt.show()

	if args['which_zens'] is 'unfolded':
		ut.pINFO('Using unfolded zenith angles')
		idxs = np.where(np.logical_and(qzen_uf>=args['zen_output_min'], qzen_uf<=args['zen_output_max']))
		qzen = qzen_uf[idxs]
		qrad = qrad_uf[idxs,:]
		qrad_err = qrad_err_uf[idxs,:]
	elif args['which_zens'] is 'folded':
		ut.pINFO('Using folded zenith angles')
		idxs = np.where(np.logical_and(qzen>=args['zen_output_min'], qzen<=args['zen_output_max']))
		#print(idxs)
		qzen = qzen[idxs]
		qrad = qrad[idxs[0],:]
		qrad_err = qrad_err[idxs[0],:]
		#print(qzen.shape)
		#print(qrad.shape)
	else:
		ut.pWARN('This option should never happen')

	f1, a1 = make_fig_axes(1,1,1)
	a1[0].plot(qzen, np.nanmedian(qrad, axis=1), marker='x')
	a1[0].fill_between(qzen, 
						np.nanmedian(qrad, axis=1) - np.nanmedian(qrad_err, axis=1), 
						np.nanmedian(qrad, axis=1) + np.nanmedian(qrad_err, axis=1), alpha=0.5,
						color='tab:orange')
	plt.show()

	if args['wavgrid'] is not None:
		output_wavgrid = args['wavgrid']
	else:
		output_wavgrid = wavgrid

	min_lat = np.min(rgn_lats)
	max_lat = np.max(rgn_lats)

	mean_lat = 0.5*(min_lat+max_lat)
	mean_lon = 0.5*(np.min(rgn_lons)+np.max(rgn_lons))
	spec_fwhm = args['spec_fwhm']
	ngeom = len(qzen)
	nconvs = [len(output_wavgrid)]*ngeom
	navs = [1]*ngeom
	fov_averaging_record = []
	spec_record = []
	for i in range(ngeom):
		fov_each_geom = []
		for j in range(navs[i]):
			fov = ( mean_lat,
					mean_lon,
					qzen[i], # solar angle = zenith
					qzen[i], # emission angle (towards observer) = zenith (approximately)
					180, # photon scatter angle (from sun towards us ~ 180 deg)
					1.0 # weight to give geometry when performing FOV averaging, we only have one FOV geometry
					)
			fov_each_geom.append(fov)
		fov_averaging_record.append(fov_each_geom)
		spec_data = np.zeros((nconvs[i],3))
		spec_data[:,0] = output_wavgrid
		spec_data[:,1] = np.nan_to_num(np.interp(output_wavgrid, wavgrid, qrad[i]), copy=False)
		spec_data[:,2] = np.nan_to_num(np.interp(output_wavgrid, wavgrid, qrad_err[i]), copy=False)
		spec_record.append(spec_data)

	spx_dict = {	'fwhm':spec_fwhm,
					'latitude':mean_lat,
					'longitude':mean_lon,
					'ngeom':ngeom,
					'nconvs':nconvs,
					'navs':navs,
					'fov_averaging_record':fov_averaging_record,
					'spec_record':spec_record}

	# if we have a *.set file to copy, then use it. else make some defaults
	if args['set_file']:
		set_dict = set_data
	else:
		set_dict = {}
		set_dict['n_zens'] = nzens
		set_dict['quad_points'] = qcos
		set_dict['quad_weights'] = qweights
		set_dict['n_fourier_components'] = 6 # overwritten in code anyway
		set_dict['n_azi_angles'] = 100 # this has been set to 100 for years, seems to be a good number
		set_dict['sunlight_flag'] = 1 # we have sunshine
		set_dict['solar_dist'] = float(hdul[0].header['SUN_DIST'])
		set_dict['lower_boundary_flag'] = 1
		set_dict['ground_albedo'] = 0.0
		set_dict['surf_temp'] = 0.0
		set_dict['base_altitude'] = -65.0 # unsure of units
		set_dict['n_atm_layers'] = 39 # 39 is highest supported for scattering
		set_dict['layer_type_flag'] = 1
		set_dict['layer_int_flag'] = 1

	containing_folder = './lat_{:+07.2f}_to_{:+07.2f}'.format(min_lat, max_lat)
	return([containing_folder], [{'spx':spx_dict, 'set':set_dict}])

def read_zenX_file(zenfile):
	from io import StringIO
	# have to quickly hack 'D' to 'E' as FORTRAN uses D to represent 10^X for doubles
	with open(zenfile, 'r') as f:
		data = ''.join(f.read()).replace('D','E')
	newf = StringIO(data)
	return(np.loadtxt(newf, skiprows=1,usecols=0))

def scale_error(hdul, factor, show_plots=True):
	# scale error
	if show_plots:
		f1, a1 = make_fig_axes(2,2,4)
		nz,ny,nx = hdul[0].data.shape
		wavgrid = fitscube.process.sinfoni.datacube_wavelength_grid(hdul[0])
		i, j = (int(nx/2), int(ny/2))
		ylims = (np.nanmin(hdul[0].data[:,j,i]), np.nanmax(hdul[0].data[:,j,i]))
		a1[0].plot(wavgrid, hdul[0].data[:,j,i])
		a1[0].plot(wavgrid, hdul[1].data[:,j,i])
		a1[0].set_title('full error before scaling')
		a1[0].set_ylim(ylims)

		a1[1].plot(wavgrid, hdul[0].data[:,j,i])
		a1[1].plot(wavgrid, hdul[11].data[:,0,0])
		a1[1].set_title('small error before scaling')
		a1[1].set_ylim(ylims)

	hdul['ERROR'].data *= factor # hdul[1]
	hdul['ERROR_VS_SPECTRAL'].data *= factor # hdul[11]

	if show_plots:
		a1[2].plot(wavgrid, hdul[0].data[:,j,i])
		a1[2].plot(wavgrid, hdul[1].data[:,j,i])
		a1[2].set_title('full error after scaling')
		a1[2].set_ylim(ylims)

		a1[3].plot(wavgrid, hdul[0].data[:,j,i])
		a1[3].plot(wavgrid, hdul[11].data[:,0,0])
		a1[3].set_title('small error after scaling')
		a1[3].set_ylim(ylims)
		plt.show()

	return(hdul)

def spectral_scale_error(hdul, spectral_factor, show_plots=True):
	# spectral_factor - an array that has the same number of entries as hdul[1] and hdul[11] has wavelengths
	# 					applies a spectrally varying factor
	if show_plots:
		f1, a1 = make_fig_axes(2,2,4)
		nz,ny,nx = hdul[0].data.shape
		wavgrid = fitscube.process.sinfoni.datacube_wavelength_grid(hdul[0])
		i, j = (int(nx/2), int(ny/2))
		ylims = (np.nanmin(hdul[0].data[:,j,i]), np.nanmax(hdul[0].data[:,j,i]))
		a1[0].plot(wavgrid, hdul[0].data[:,j,i])
		a1[0].plot(wavgrid, hdul[1].data[:,j,i])
		a1[0].set_title('full error before spectral scaling')
		a1[0].set_ylim(ylims)

		a1[1].plot(wavgrid, hdul[0].data[:,j,i])
		a1[1].plot(wavgrid, hdul[11].data[:,0,0])
		a1[1].set_title('small error before spectral scaling')
		a1[1].set_ylim(ylims)

	hdul['ERROR'].data *= spectral_factor[:, None,None]
	hdul['ERROR_VS_SPECTRAL'].data *= spectral_factor[:,None,None]

	if show_plots:
		a1[2].plot(wavgrid, hdul[0].data[:,j,i])
		a1[2].plot(wavgrid, hdul[1].data[:,j,i])
		a1[2].set_title('full error after spectral scaling')
		a1[2].set_ylim(ylims)

		a1[3].plot(wavgrid, hdul[0].data[:,j,i])
		a1[3].plot(wavgrid, hdul[11].data[:,0,0])
		a1[3].set_title('small error after spectral scaling')
		a1[3].set_ylim(ylims)
		plt.show()

	return(hdul)

def telluric_transmittance_to_error_spec_factor(telluric_transmittance_file, wavgrid):
	ttd = np.loadtxt(telluric_transmittance_file)
	dw = np.min(wavgrid[1:]-wavgrid[:-1])
	wg, tt = subsample.conv(ttd[:,0], ttd[:,1], dw, conv_type='norm_top_hat', outgrid=wavgrid)
	return(1.0/tt)

def make_fig_axes(nx,ny,n,sx=12,sy=12):
	f1 = plt.figure(figsize=[x/2.54 for x in (sx*nx, sy*ny)])
	a1 = []
	for i in range(n):
		a1.append(f1.add_subplot(ny,nx,i+1))
	return(f1, a1)

def get_unique_path_part(paths):
	#print(paths)
	common_prefix = os.path.commonpath(paths)
	common_suffix = os.path.commonpath([p[::-1] for p in paths])[::-1]
	print(common_prefix)
	print(common_suffix)
	unique_path_part = []
	for p in paths:
		unique_path_part.append('.'+p[len(common_prefix):len(p)-(len(common_suffix))])
	return(unique_path_part)

def parse_args(argv):
	"""Parses command line arguments, see https://docs.python.org/3/library/argparse.html"""
	import argparse as ap
	# =====================
	# FORMATTER INFORMATION
	# ---------------------
	# A formatter that inherits from multiple formatter classes has all the attributes of those formatters
	# see https://docs.python.org/3/library/argparse.html#formatter-class for more information on what each
	# of them do.
	# Quick reference:
	# ap.RawDescriptionHelpFormatter -> does not alter 'description' or 'epilog' text in any way
	# ap.RawTextHelpFormatter -> Maintains whitespace in all help text, except multiple new lines are treated as one
	# ap.ArgumentDefaultsHelpFormatter -> Adds a string at the end of argument help detailing the default parameter
	# ap.MetavarTypeHelpFormatter -> Uses the type of the argument as the display name in help messages
	# =====================	
	class RawDefaultTypeFormatter(ap.RawDescriptionHelpFormatter, ap.ArgumentDefaultsHelpFormatter, ap.MetavarTypeHelpFormatter):
		pass
	class RawDefaultFormatter(ap.RawDescriptionHelpFormatter, ap.ArgumentDefaultsHelpFormatter):
		pass
	class TextDefaultTypeFormatter(ap.RawTextHelpFormatter, ap.ArgumentDefaultsHelpFormatter, ap.MetavarTypeHelpFormatter):
		pass
	class TextDefaultFormatter(ap.RawTextHelpFormatter, ap.ArgumentDefaultsHelpFormatter):
		pass

	#parser = ap.ArgumentParser(description=__doc__, formatter_class = ap.TextDefaultTypeFormatter, epilog='END OF USAGE')
	# ====================================
	# UNCOMMENT to enable block formatting
	# ------------------------------------
	parser = ap.ArgumentParser	(	description=ut.str_block_indent_raw(ut.str_rationalise_newline_for_wrap(__doc__), wrapsize=79),
									formatter_class = RawDefaultTypeFormatter,
									epilog=ut.str_block_indent_raw(ut.str_rationalise_newline_for_wrap('END OF USAGE'), wrapsize=79)
								)
	# ====================================

	parser.add_argument('target_cubes', type=str, nargs='+',  help='fitscubes to operate on, ')
	
	parser.add_argument('--slurm_dir', type=str, help='Will write slurm submission scripts and directory trees to this directory, cubes will be given a unique directory within this one', 
						default=os.path.expanduser('~/scratch/slurm/nemesis'))
	parser.add_argument('--show_plots', action='store_true', help='Should we display intermediate plots?')
	parser.add_argument('--require_relative', type=str, nargs='+', help='If present, will ensure that ALL the files (relative to the target cube) are present before operating', default=[])
	
	#parser.add_argument('--copy_nemesis_inputs_path', type=str, help='Path to a script that will copy all necessary input files for a nemesis run', 
	#					default=os.path.expanduser('~/Documents/code/python3/standalone/copy_nemesis_inputs.py'))
	parser.add_argument('--nemesis_templates', type=str, nargs='+', help='Directory that contains the template files for this nemesis run, will copy supporting files from this location', 
						default=[os.path.expanduser('~/scratch/nemesis_run_results/nemesis_run_templates/neptune/cloud_and_ch4_2')])
	parser.add_argument('--telluric_reference_file', type=str, help='A file containing data on atmospheric transmission covering the wavelength range we are interested in',
						default=os.path.expanduser('~/Documents/reference_data/telluric_features/mauna_kea/sky_transmission/mktrans_zm_30_15.dat'))

	parser.add_argument('--data_err_scaling', type=float, help='Scale the error on the data by this value, use this to tune the error for the NEMESIS runs', default=1)
	parser.add_argument('--error_from_smooth', action='store_true', help='If present, when the spectrum is smoothed will find the standard deviation of (spec - smoothed_spec) and add it to the error')

	parser.add_argument('--zen_output_min', type=float, help='The minimum zenith angle to output', default=0)
	parser.add_argument('--zen_output_max', type=float, help='The maximum zenith angle to output', default=80)
	parser.add_argument('--n_zen_quadrature', type=int, help='the number of zenith quadrature points to compute',
						default=9)
	parser.add_argument('--which_zens', type=str, choices=['folded','unfolded'], help='Should we unfold or fold zenith angles?', 
						default='folded')
	parser.add_argument('--set_file', type=str, help='A <runname>.set file to get quadrature points from')
	
	parser.add_argument('--wavgrid_file', type=str, help='A file containing a wavelength grid to interpolate to')
	parser.add_argument('--wavgrid_n', type=int, help='Number of points in between wavgrid_min and wavgrid_max')
	parser.add_argument('--wavgrid_min', type=float, help='Lower bound of wavelength grid')
	parser.add_argument('--wavgrid_max', type=float, help='Upper bound of wavelength grid')
	parser.add_argument('--wavgrid_step', type=float, help='Step size of wavelength grid')

	parser.add_argument('--spec_fwhm', type=float, help='FWHM of the square box to convolve with the calculated spectrum. -ve if providing a *.fil file, 0 if using channel integrated K-tables, +ve otherwise', default=0)

	parser.add_argument('--runname', type=str, help='name of nemesis run to output to, combined with slurm_dir and unique name', default='./neptune')

	# get type of region/retrieval we want to work on
	parser.add_argument('--limb_darkening_type', type=str, choices=('simple', 'minnaert', 'none'), help='What type of limb darkening should we create an run-ensemble for?', default='none')	
	parser.add_argument('--limb_darkening.full_disk', action='store_true', help='If present, will use the complete disk as the region to compute limb darkening over')
	parser.add_argument('--limb_darkening.max_zen', type=float, nargs='?', const=60.0,  help='If present, will use a restricted set of zenith angles to compute minnaert limb darkening parameters.')
	
	# get the retrieval region to operate on
	parser.add_argument('--region.pixel_box', type=int, nargs=4, help='If present will grab a range of pixels specified in the format "minx,miny,maxx,maxy". Note that due to differences with 0 vs 1 indexing, the corresponding pixel region in QFitsView will have 1 added to each value.', default=None)

	# how is the region interpreted?
	parser.add_argument('--region.as_set', action='store_true', help='If present, will treat a region as a set of pixels to run individual retrievals for instead of averaging them together')

	# if we are slicing via latitude, how is that working?
	parser.add_argument('--latitude.min', type=float, help='Minimum latitude to consider', default=-90)
	parser.add_argument('--latitude.max', type=float, help='Maximum latitude to consider', default=90)
	parser.add_argument('--latitude.n', type=int, help='Number of chunks to split latitude into (NOT the number of boundaries), if using "micro stepping" will actually create n-1 bins but there will be n+1 bin boundaries.', default=36)
	parser.add_argument('--latitude.micro_step', action='store_true', help='If present, latitude bins will be "micro stepped", i.e. their step with be half the bin width (0->10, 5->15, 10->20, etc.). If not present, bin step will be the same as the bin width (0->10, 10->20, 20->30, etc.).')
	parser.add_argument('--latitude.bin_step', type=float, help='step between latitude bins (leave unset to automatically work it out). Note, if both this and "--latitude.bin_width" are present, will ignore "--latitude.n" and "--latitude.micro_step".', default=None)
	parser.add_argument('--latitude.bin_width', type=float, help='width of latitude bins (leave unset to automatically work it out). Note, if both this and "--latitude.bin_step" are present, will ignore "--latitude.n" and "--latitude.micro_step".', default=None)

	parser.add_argument('--no_telluric_error_scaling', action='store_false', dest='telluric_error_scaling', 
																help='If present, *.spx files that are created will not have their error scaled by atmospheric transmission')
	parser.add_argument('--base_fractional_error', type=float, help='The smallest fraction of the spectral radiance the error on the spectral radiance can be. Set to 0 to disable.', default=0.1)

	# arguments that control cloud masking
	parser.add_argument('--cloud_mask.disable', action='store_false', dest='cloud_mask.enable', help='If present, will not mask out cloulds before calculating limb darkening')
	parser.add_argument('--cloud_mask.load', nargs='?', type=str, default=None, const='./mask.fits', help='If present will load a mask from the file "./mask.fits" relative to the target cube instead of creating one using "--cloud_mask.mode". My pass a different filename')
	
	parser.add_argument('--cloud_mask.save', nargs='?', type=str, default=None, const='./mask.fits', help='If present, will save a mask in the file "./mask.fits" that is relative to the target cube. May pass a different filename')
	cloud_mask_mode_choices=['adaptive', 'additive', 'static', 'manual']
	parser.add_argument('--cloud_mask.mode', type=str, default='manual', choices=cloud_mask_mode_choices, help=f'How is the cloud mask calculated, choices are {cloud_mask_mode_choices}')
	parser.add_argument('--cloud_mask.edit', action='store_true', help='If present will edit any loaded or created mask using the manual mask editor.')

	# arguments that control copying from "nemesis_templates"
	parser.add_argument('--copy.src.runname', type=str, help='runname of the NEMESIS files to copy to', default='neptune')
	parser.add_argument('--copy.dest.no_overwrite', action='store_false', dest='copy.dest.overwrite', help='If present, will not overwrite files at destination')
	parser.add_argument('--copy.src.n_search_parents', type=int, help='Sets the number of parent directories that will be searched for NEMESIS input files not found in the current directory', default=5)

	nemesis_tags = {'LBL':'Layer by layer calcuation',
					'LIMB':'Calculation is of a limb',
					'GP':'Calculation is of a giant planet',
					'REF':'Calcualtion is of a reflectance spectrum',
					'TRA':'Calculation is of a transit spectrum',
					'SCA':'Calculation includes scattering',
					'CHI':'Channel integratons are required',
					'RFL':'Include extra reflecting layer calculations the output',
					'VPF':'Include a file that limits specified gases volume mixing ratio by saturation',
					'RDW':'Includes a file that contains ranked wavelengths, NEMESIS will fit highest ranked wavelengths first, reduces computational time, incompatible with SCA',
					'ZEN':'Includes a file that specifies where the zenith angle is measured from, if not present zenith angle is measured from bottom of deepest layer'
					}
					
	parser.add_argument('--copy.nemesis.tags', type=str, nargs='+', choices=nemesis_tags.keys(), 
						help=f'What type of run are you performing (changes files to be copied)\n{ut.str_dict(nemesis_tags)}', default=['SCA','GP','REF'])
	parser.add_argument('--copy.dry_run', action='store_true', help='If present, will not actually copy files')



	print(argv)

	# Actually parse arguments
	parsed_args = vars(parser.parse_args(argv)) # I prefer a dictionary interface

	# ================
	# Filter arguments
	# ----------------
	# filter target cubes to only include those where all required files are present
	chosen_target_cubes = []
	for tc in parsed_args['target_cubes']:
		required_files_present = [os.path.exists(os.path.join(os.path.dirname(tc),req_rel)) for req_rel in parsed_args['require_relative']]
		if all(required_files_present):
			chosen_target_cubes.append(os.path.abspath(tc))
	parsed_args['target_cubes'] = chosen_target_cubes

	# use "--wavgrid_XXX" arguments to find a wavelength grid to interpolate to
	if parsed_args['wavgrid_file']:
		parsed_args['wavgrid'] = np.loadtxt(parsed_args['wavgrid_file'], usecols=0)
	elif not any([x is None for x in (parsed_args['wavgrid_n'],parsed_args['wavgrid_min'],parsed_args['wavgrid_max'])]):
		parsed_args['wavgrid'] = np.linspace(	parsed_args['wavgrid_min'],
												parsed_args['wavgrid_max'],parsed_args['wavgrid_n'])
	elif not any([x is None for x in (parsed_args['wavgrid_step'],
									parsed_args['wavgrid_min'],
									parsed_args['wavgrid_max'])]):
		parsed_args['wavgrid'] = np.arange(	parsed_args['wavgrid_min'], 
											parsed_args['wavgrid_max']+1E-6*parsed_args['wavgrid_step'], 
											parsed_args['wavgrid_step'])
	else:
		ut.pINFO('Not enough "--wavgrid_XXX" arguments passed to specify a wavelength grid to interpolate to')
		parsed_args['wavgrid'] = None

	# change "none" as a string to None value when appropriate
	if parsed_args['limb_darkening_type'].lower()  == 'none':
		parsed_args['limb_darkening_type'] = None
	
	# cull nemesis templates to only include folders
	temp = []
	for item in parsed_args['nemesis_templates']:
		if os.path.isdir(item):
			temp.append(item)
	parsed_args['nemesis_templates'] = temp
	# ================

	return(parsed_args)

if __name__=='__main__':
	main(sys.argv[1:])
